

# TP dâ€™introduction Ã  PySpark

## Objectifs du TP

* Installer et configurer PySpark dans un environnement Python.
* CrÃ©er une `SparkSession`.
* Comprendre les notions clÃ©s :

  * RDD vs DataFrame
  * Partitions et rÃ©partition
  * Cache / persist
  * Broadcast variables
  * Paradigme masterâ€“worker
  * TolÃ©rance aux pannes

* Manipuler un DataFrame Spark :

  * Filtres (`filter`)
  * SÃ©lections (`select`)
  * AgrÃ©gations (`groupBy`)
  * Joins (`join`)

---

# Installation et configuration de PySpark

## ðŸ”§ CrÃ©er un environnement virtuel Python

```bash
python -m venv env
source env/bin/activate
```

## ðŸ”§ Installer PySpark

```bash
pip install pyspark
```

Avec cette installation, **les binaires Spark sont automatiquement intÃ©grÃ©s dans le package PySpark**, donc aucune installation sÃ©parÃ©e dâ€™Apache Spark nâ€™est nÃ©cessaire.

---

# Votre premiÃ¨re session Spark

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkConfigured") \
    .getOrCreate()
```

---

# Concepts essentiels de Spark

## RDD vs DataFrame

| Concept      | RDD                              | DataFrame                       |
| ------------ | -------------------------------- | ------------------------------- |
| API          | Bas niveau                       | Haut niveau                     |
| Typage       | Non structurÃ©                    | StructurÃ© (schÃ©ma)              |
| Optimisation | Aucune, transformations directes | OptimisÃ© par le moteur Catalyst |
| Usage        | FlexibilitÃ© maximale             | Performance, SQL, analytics     |

Pour 99% des cas : **DataFrame**.
Le dataFrame est une structure de donnÃ©e splus moderne et plus efficace dans notre contexte.

---

## Partitions & rÃ©partition

* Spark **divise** les donnÃ©es en **partitions**.
* Chaque partition est traitÃ©e en parallÃ¨le par un worker.
* RÃ©partition = placement optimal des partitions selon les opÃ©rations.
* Repartition vs coalesce :

  * `df.repartition(n)` â†’ reshuffle complet (lent)
  * `df.coalesce(n)` â†’ fusion sans reshuffle (rapide)

* Consultez les docstrings de ces fonctions, faites un bref rÃ©sumÃ© de ce que vous en avez compris.
---

## Paradigme masterâ€“worker

* **Driver (master)** :
  programme Python qui soumet des tÃ¢ches
* **Workers (executors)** :
  machines exÃ©cutant les transformations sur les partitions

---

## TolÃ©rance aux pannes

Si un worker meurt :

* Spark rÃ©cupÃ¨re la partition depuis sa source
* rejoue les transformations
* redÃ©ploie la tÃ¢che sur un autre worker
=> Le modÃ¨le est **rÃ©silient par conception**.

---

# Partie pratique

Nous utiliserons un dataset simple : **Spotify**

## Dataset proposÃ©

Le dataset se compose d'infos sur des chansons
https://www.kaggle.com/datasets/kapturovalexander/spotify-data-from-pyspark-course/data

## Exercices pratiques


### 1. Charger le CSV

```python
df = spark.read.csv("spotify-data.csv", header=True, inferSchema=True)
df.show()
df.printSchema()
```
Expliquez ce que fait ce code :

```python
decades_data = [
    (y, f"{y//10 * 10}s")
    for y in range(1920, 2021)
]
decades_df = spark.createDataFrame(decades_data, ["year", "decade_name"])

decades_df.show(5)
```

### 2. Questions mÃ©tier

**Q1**	Quelles sont les chansons publiÃ©es aprÃ¨s 2015 qui ont un score de PopularitÃ© supÃ©rieur Ã  85 ?  
**Q2**	Quelles sont les chansons qui ne sont ni explicites (explicit = 0) ni instrumentales (instrumentalness = 0) ?  
**Q3**	Quels titres sont trÃ¨s "dansables" (danceability > 0.8) OU trÃ¨s "positifs" (valence > 0.8) ?  
**Q4**	Calculer la durÃ©e moyenne des chansons (duration_ms) pour chaque annÃ©e de sortie.  

Exemple de code pour la Q1 :  

```python
popular_recent_songs = df_spotify.filter(
    (col("year") > 2016) & (col("popularity") < 15)
)
popular_recent_songs.select("name", "main_artist", "year", "popularity").show(5)
```  

Example de code pour la Q4 :  

```python
from pyspark.sql.functions import avg

avg_duration_by_year = df_spotify.groupBy("year").agg(
    avg("speechiness").alias("avg_speechiness")
).orderBy("year")

avg_speechiness_by_year.show(5)
```

**Q5**	Quel est l'artiste principal (main_artist) qui possÃ¨de le plus grand nombre de titres dans le dataset ?  
**Q6**	Quelles sont les caractÃ©ristiques moyennes (energy, acousticness) des titres en Mode majeur (mode = 1) par rapport au Mode mineur (mode = 0) ?  
**Q7**	Trouver l'annÃ©e oÃ¹ le score de Loudness (volume) moyen est le plus faible et l'annÃ©e oÃ¹ il est le plus Ã©levÃ©.  
**Q8**	Associer chaque chanson Ã  sa DÃ©cennie de sortie en utilisant le DataFrame de rÃ©fÃ©rence decades_df crÃ©Ã© en amont.   



Exemple de code pour Q8 :  

```python
# Jointure interne du DataFrame Spotify et du DataFrame DÃ©cennies sur la colonne 'year'
df_with_decades = df_spotify.join(
    decades_df,
    on="year",
    how="inner" # Jointure interne : on ne garde que les correspondances
)

df_with_decades.select("name", "main_artist", "year", "decade_name").show(5, truncate=False)
```  


**Q9**	AprÃ¨s avoir trouvÃ© l'artiste principal le plus populaire (selon la popularitÃ© moyenne), afficher tous les titres de cet artiste uniquement.  

**Q10**	Calculer l'Ã©cart entre la PopularitÃ© de chaque chanson et la PopularitÃ© moyenne de l'annÃ©e de sortie de cette chanson.  



---

N'oubliez pas d'arrÃªter la SparkSession avec :

```python
spark.stop()a()
```