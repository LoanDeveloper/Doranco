#!/usr/bin/env python3
"""
Script pour visualiser la charge de travail PySpark via Spark UI.
Spark UI sera accessible sur http://localhost:4040 pendant l'exÃ©cution.
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count, max, min, desc, asc
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
import time

print("=" * 60)
print("ğŸš€ DÃ©marrage de Spark avec Spark UI...")
print("=" * 60)

# CrÃ©er SparkSession avec configuration pour Spark UI
spark = SparkSession.builder \
    .appName("TP_PySpark_Spotify_UI_Demo") \
    .config("spark.ui.enabled", "true") \
    .config("spark.ui.port", "4040") \
    .getOrCreate()

spark_ui_url = spark.sparkContext.uiWebUrl
print(f"\nâœ… SparkSession crÃ©Ã©e avec succÃ¨s!")
print(f"ğŸ“Š Spark UI disponible sur: {spark_ui_url}")
print("\nâ³ Ouvrez cette URL dans votre navigateur pour visualiser les Jobs, Stages, et ExÃ©cuteurs.")
print("=" * 60)

# DÃ©finir le schÃ©ma
schema = StructType([
    StructField("id", StringType(), True),
    StructField("name", StringType(), True),
    StructField("artists", StringType(), True),
    StructField("duration_ms", IntegerType(), True),
    StructField("release_date", StringType(), True),
    StructField("year", IntegerType(), True),
    StructField("acousticness", DoubleType(), True),
    StructField("danceability", DoubleType(), True),
    StructField("energy", DoubleType(), True),
    StructField("instrumentalness", DoubleType(), True),
    StructField("liveness", DoubleType(), True),
    StructField("loudness", DoubleType(), True),
    StructField("speechiness", DoubleType(), True),
    StructField("tempo", DoubleType(), True),
    StructField("valence", DoubleType(), True),
    StructField("mode", IntegerType(), True),
    StructField("key", IntegerType(), True),
    StructField("popularity", IntegerType(), True),
    StructField("explicit", IntegerType(), True)
])

print("\nğŸ“ Chargement du dataset spotify-data.csv...")
df_spotify = spark.read.csv("spotify-data.csv", header=True, schema=schema)
df_spotify.cache()  # Cache pour de meilleures performances

print(f"ğŸ“ˆ Nombre total de lignes: {df_spotify.count()}")

# ExÃ©cuter les opÃ©rations du TP pour gÃ©nÃ©rer du travail
print("\n" + "=" * 60)
print("ğŸ”„ ExÃ©cution des requÃªtes du TP...")
print("=" * 60)

# Q1: Chansons publiÃ©es aprÃ¨s 2015 avec popularitÃ© > 85
print("\nğŸ“‹ Q1: Filtrage des chansons populaires rÃ©centes...")
popular_recent = df_spotify.filter((col("year") > 2015) & (col("popularity") > 85))
count_q1 = popular_recent.count()
print(f"   â¡ï¸ {count_q1} chansons trouvÃ©es")

# Q2: Non explicites et non instrumentales
print("\nğŸ“‹ Q2: Filtrage non explicites et non instrumentales...")
non_explicit = df_spotify.filter((col("explicit") == 0) & (col("instrumentalness") == 0))
count_q2 = non_explicit.count()
print(f"   â¡ï¸ {count_q2} chansons trouvÃ©es")

# Q3: TrÃ¨s dansables OU trÃ¨s positives
print("\nğŸ“‹ Q3: Filtrage dansables ou positives...")
danceable = df_spotify.filter((col("danceability") > 0.8) | (col("valence") > 0.8))
count_q3 = danceable.count()
print(f"   â¡ï¸ {count_q3} chansons trouvÃ©es")

# Q4: DurÃ©e moyenne par annÃ©e
print("\nğŸ“‹ Q4: AgrÃ©gation durÃ©e moyenne par annÃ©e...")
avg_duration = df_spotify.groupBy("year").agg(avg("duration_ms").alias("avg_duration_ms")).orderBy("year")
avg_duration.collect()
print("   â¡ï¸ AgrÃ©gation complÃ©tÃ©e")

# Q5: Artiste le plus prolifique
print("\nğŸ“‹ Q5: Comptage par artiste...")
artist_count = df_spotify.groupBy("artists").agg(count("*").alias("nombre_titres")).orderBy(desc("nombre_titres"))
top_artist = artist_count.first()
print(f"   â¡ï¸ Top artiste: {top_artist['artists']} avec {top_artist['nombre_titres']} titres")

# Q6: CaractÃ©ristiques par mode
print("\nğŸ“‹ Q6: CaractÃ©ristiques moyennes par mode...")
by_mode = df_spotify.groupBy("mode").agg(
    avg("energy").alias("avg_energy"),
    avg("acousticness").alias("avg_acousticness")
).orderBy("mode")
by_mode.collect()
print("   â¡ï¸ AgrÃ©gation complÃ©tÃ©e")

# Q7: Loudness par annÃ©e
print("\nğŸ“‹ Q7: Loudness par annÃ©e...")
loudness_by_year = df_spotify.groupBy("year").agg(avg("loudness").alias("avg_loudness")).orderBy("avg_loudness")
loudness_by_year.collect()
print("   â¡ï¸ AgrÃ©gation complÃ©tÃ©e")

# Q8: Jointure avec dÃ©cennie
print("\nğŸ“‹ Q8: CrÃ©ation et jointure avec les dÃ©cennies...")
decades_data = [(y, f"{y//10 * 10}s") for y in range(1920, 2021)]
decades_df = spark.createDataFrame(decades_data, ["year", "decade_name"])
songs_with_decades = df_spotify.join(decades_df, "year", "left")
songs_with_decades.select("name", "year", "decade_name").show(5, truncate=False)

# Q9: Top 10 par dÃ©cennie
print("\nğŸ“‹ Q9: Top 10 chansons par dÃ©cennie...")
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number
window_spec = Window.partitionBy("decade_name").orderBy(desc("popularity"))
top_by_decade = songs_with_decades.withColumn("rank", row_number().over(window_spec)).filter(col("rank") <= 10)
top_by_decade.select("decade_name", "name", "popularity", "rank").orderBy("decade_name", "rank").show(20, truncate=False)

# Q10: Statistiques par dÃ©cennie
print("\nğŸ“‹ Q10: Statistiques complÃ¨tes par dÃ©cennie...")
decade_stats = songs_with_decades.groupBy("decade_name").agg(
    count("*").alias("total_songs"),
    avg("popularity").alias("avg_popularity"),
    avg("danceability").alias("avg_danceability"),
    avg("energy").alias("avg_energy")
).orderBy("decade_name")
decade_stats.show()

print("\n" + "=" * 60)
print("âœ… Toutes les opÃ©rations sont terminÃ©es!")
print(f"ğŸ“Š Consultez Spark UI sur: {spark_ui_url}")
print("=" * 60)
print("\nğŸ’¡ L'interface montrera:")
print("   - Jobs: Les 10 opÃ©rations du TP")
print("   - Stages: Le dÃ©tail de chaque Ã©tape (map, reduce, shuffle)")
print("   - Storage: Le DataFrame en cache")
print("   - Executors: Les ressources utilisÃ©es")
print("\nğŸ”´ Appuyez sur Ctrl+C pour arrÃªter Spark et fermer l'UI...")

try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    print("\n\nâ¹ï¸ ArrÃªt de Spark...")
    spark.stop()
    print("ğŸ‘‹ SparkSession terminÃ©e.")
