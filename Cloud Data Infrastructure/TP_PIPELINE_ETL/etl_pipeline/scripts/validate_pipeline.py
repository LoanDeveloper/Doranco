#!/usr/bin/env python3
"""
Script de validation du pipeline ETL
"""

import os
import pandas as pd
import sqlite3
import logging

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def validate_data_lake():
    """Validation de la zone curated du Data Lake"""
    logger.info("Validation de la zone curated...")
    
    curated_path = "etl_star_schema_dataset/etl_star_schema/data_lake/curated"
    parquet_file = os.path.join(curated_path, "orders_clean.parquet")
    
    if not os.path.exists(parquet_file):
        logger.error(f"Fichier curated introuvable: {parquet_file}")
        return False
    
    try:
        df = pd.read_parquet(parquet_file)
        logger.info(f"Fichier curated valid√©: {len(df)} lignes, {len(df.columns)} colonnes")
        logger.info(f"Colonnes: {list(df.columns)}")
        return True
    except Exception as e:
        logger.error(f"Erreur de lecture du fichier curated: {e}")
        return False

def validate_warehouse():
    """Validation du Data Warehouse"""
    logger.info("Validation du Data Warehouse...")
    
    warehouse_path = "etl_star_schema_dataset/etl_star_schema/warehouse.db"
    
    try:
        conn = sqlite3.connect(warehouse_path)
        cursor = conn.cursor()
        
        # V√©rification des tables
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
        tables = [table[0] for table in cursor.fetchall()]
        
        expected_tables = ['fact_sales', 'dim_customer', 'dim_product', 'dim_time', 'dim_currency']
        missing_tables = [table for table in expected_tables if table not in tables]
        
        if missing_tables:
            logger.error(f"Tables manquantes dans le Data Warehouse: {missing_tables}")
            return False
        
        # V√©rification des donn√©es dans fact_sales
        cursor.execute("SELECT COUNT(*) FROM fact_sales;")
        count = cursor.fetchone()[0]
        logger.info(f"Table fact_sales: {count} lignes")
        
        # V√©rification de quelques enregistrements
        cursor.execute("SELECT * FROM fact_sales LIMIT 3;")
        sample_data = cursor.fetchall()
        logger.info(f"√âchantillon de donn√©es: {sample_data}")
        
        conn.close()
        logger.info("Data Warehouse valid√© avec succ√®s")
        return True
        
    except Exception as e:
        logger.error(f"Erreur de validation du Data Warehouse: {e}")
        return False

def validate_data_quality():
    """Validation de la qualit√© des donn√©es"""
    logger.info("Validation de la qualit√© des donn√©es...")
    
    try:
        # Chargement des donn√©es transform√©es
        df = pd.read_parquet("etl_star_schema_dataset/etl_star_schema/data_lake/curated/orders_clean.parquet")
        
        # V√©rifications de qualit√©
        checks = []
        
        # 1. Pas de valeurs manquantes
        missing_values = df.isnull().sum().sum()
        checks.append(("Valeurs manquantes", missing_values == 0))
        
        # 2. Types de donn√©es corrects
        expected_types = {
            'order_id': 'int64',
            'customer_id': 'int64', 
            'product_id': 'int64',
            'quantity': 'int64',
            'unit_price': 'float64',
            'total_amount': 'float64'
        }
        
        type_checks = []
        for col, expected_type in expected_types.items():
            if col in df.columns:
                actual_type = str(df[col].dtype)
                type_checks.append(actual_type == expected_type)
        
        checks.append(("Types de donn√©es", all(type_checks)))
        
        # 3. Coh√©rence des montants
        df['calculated_check'] = df['quantity'] * df['unit_price']
        discrepancy_count = (abs(df['total_amount'] - df['calculated_check']) > 0.01).sum()
        checks.append(("Coh√©rence des montants", discrepancy_count == 0))
        
        # 4. Dates valides
        date_check = df['order_date'].notna().all()
        checks.append(("Dates valides", date_check))
        
        # Affichage des r√©sultats
        for check_name, result in checks:
            status = "‚úì" if result else "‚úó"
            logger.info(f"{status} {check_name}: {'OK' if result else '√âCHEC'}")
        
        all_passed = all(result for _, result in checks)
        logger.info(f"Validation de la qualit√©: {'SUCCESS' if all_passed else '√âCHEC'}")
        return all_passed
        
    except Exception as e:
        logger.error(f"Erreur de validation de la qualit√©: {e}")
        return False

def main():
    """Ex√©cution des validations"""
    logger.info("D√©but de la validation du pipeline ETL")
    
    validations = [
        ("Data Lake", validate_data_lake),
        ("Data Warehouse", validate_warehouse),
        ("Qualit√© des donn√©es", validate_data_quality)
    ]
    
    results = []
    for name, validation_func in validations:
        logger.info(f"\n--- Validation: {name} ---")
        result = validation_func()
        results.append((name, result))
    
    # R√©sum√©
    logger.info("\n=== R√©sum√© de la validation ===")
    all_passed = True
    for name, result in results:
        status = "‚úì SUCCESS" if result else "‚úó √âCHEC"
        logger.info(f"{status} - {name}")
        if not result:
            all_passed = False
    
    if all_passed:
        logger.info("\nüéâ Toutes les validations ont r√©ussi !")
        return True
    else:
        logger.error("\n‚ùå Certaines validations ont √©chou√©")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)